{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJ3wMLs4l15bYLIbcvmqyB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TayTechie07/AI-Paper-Summarizer/blob/main/AI_paper_summarizer_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlZKh7ngvcFR",
        "outputId": "aaf2d027-9324-4a54-8fcf-5d447a0473b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzFGT5sMr8_z",
        "outputId": "4058e0ef-4249-40fa-a83d-299f28fe4f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== AI Paper Summarizer ===\n",
            "\n",
            "Enter paper abstract (or 'quit' to exit):\n",
            "Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.\n",
            "Enter your query (keywords you're interested in) or press enter to skip:\n",
            "ai or machine learnning\n",
            "\n",
            "--- Generated Summary ---\n",
            "RedSage is an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks.\n",
            "-------------------------\n",
            "\n",
            "Enter paper abstract (or 'quit' to exit):\n",
            "quit\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the summarization model\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "print(\"=== AI Paper Summarizer ===\")\n",
        "\n",
        "while True:\n",
        "    # Ask user to input abstract or text\n",
        "    text = input(\"\\nEnter paper abstract (or 'quit' to exit):\\n\")\n",
        "    if text.lower() == \"quit\":\n",
        "        break\n",
        "\n",
        "    # Ask user for a query (optional, you can highlight keywords later)\n",
        "    query = input(\"Enter your query (keywords you're interested in) or press enter to skip:\\n\")\n",
        "\n",
        "    # Generate summary\n",
        "    summary = summarizer(text, max_length=100, min_length=30, do_sample=False)\n",
        "    print(\"\\n--- Generated Summary ---\")\n",
        "    print(summary[0]['summary_text'])\n",
        "    print(\"-------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y5bPh-KFsDdN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}